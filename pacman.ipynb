{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pacman.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gFqAJcv3eno",
        "colab_type": "code",
        "outputId": "4c2cde0d-f59f-4f66-a1ec-fdf56f829f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-24 15:30:20--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.232.181.106, 34.206.253.53, 34.206.9.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.232.181.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  9.64MB/s    in 0.5s    \n",
            "\n",
            "2018-12-24 15:30:20 (9.64 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPJU256v6Lnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ksBjaTv39Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './tb'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmFSl7t74Idc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XjUglXB4K6Q",
        "colab_type": "code",
        "outputId": "5c00025c-ff39-465a-80c8-b72bad018e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://63adc34e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaAjvLH26ldp",
        "colab_type": "code",
        "outputId": "fba0e581-788a-4892-91c6-dcb4758256bf",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ade85123-92d5-48d1-9f9f-691198b5e44f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ade85123-92d5-48d1-9f9f-691198b5e44f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving conv.py to conv.py\n",
            "User uploaded file \"conv.py\" with length 2087 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7QmkNd9_4Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Dec 24 09:53:02 2018\n",
        "\n",
        "@author: maheryatim\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\"\"\"\"\"\"\"\"\"\n",
        "From OpenAI Baselines\n",
        "\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "def intprod(x):\n",
        "    return int(np.prod(x))\n",
        "\n",
        "\n",
        "def conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=\"SAME\", dtype=tf.float32, collections=None,\n",
        "           summary_tag=None):\n",
        "    with tf.variable_scope(name):\n",
        "        stride_shape = [1, stride[0], stride[1], 1]\n",
        "        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[3]), num_filters]\n",
        "\n",
        "        fan_in = intprod(filter_shape[:3])\n",
        "        \n",
        "        fan_out = intprod(filter_shape[:2]) * num_filters\n",
        "        \n",
        "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
        "\n",
        "        w = tf.get_variable(\"W\", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n",
        "                            collections=collections)\n",
        "        b = tf.get_variable(\"b\", [1, 1, 1, num_filters], initializer=tf.zeros_initializer(),\n",
        "                            collections=collections)\n",
        "\n",
        "        if summary_tag is not None:\n",
        "            tf.summary.image(summary_tag,\n",
        "                             tf.transpose(tf.reshape(w, [filter_size[0], filter_size[1], -1, 1]),\n",
        "                                          [2, 0, 1, 3]),\n",
        "                             max_images=10)\n",
        "\n",
        "        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n",
        "    \n",
        "\n",
        "def conv_(inputs):\n",
        "    \n",
        "    x = tf.nn.relu(conv2d(inputs, 32, \"l1\", [8, 8], [4, 4], pad=\"VALID\"))\n",
        "    x = tf.nn.relu(conv2d(x, 64, \"l2\", [4, 4], [2, 2], pad=\"VALID\"))\n",
        "    #x = tf.nn.relu(conv2d(x, 64, \"l3\", [2, 2], [1, 1], pad=\"VALID\"))\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "   \n",
        "    return x\n",
        "\n",
        "def conv_to_rnn(inputs, batch, seq, tboard = False): \n",
        "    '''\n",
        "    Convolution network followed by an lstm network\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    x = conv_(inputs)           \n",
        "    \n",
        "    image = x[0:1, :, :, 0:16]\n",
        "    image = tf.transpose(image, perm=[3,1,2,0])\n",
        "    tf.summary.image('Image_output_conv1', image)\n",
        "    \n",
        "    \n",
        "    s = x.get_shape().as_list()\n",
        "    print(s)\n",
        "    x = tf.reshape(x, [batch, seq, 5184])\n",
        "\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhgyxoJA0Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Dec 24 11:12:36 2018\n",
        "\n",
        "@author: maheryatim\n",
        "\"\"\"\n",
        "import numpy as np \n",
        "\n",
        "class SumTree():\n",
        "    \"\"\"\n",
        "    This SumTree code is modified version of Morvan Zhou: \n",
        "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
        "    \"\"\"\n",
        "    data_pointer = 0\n",
        "     \n",
        "    \"\"\"\n",
        "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, timesteps):\n",
        "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
        "        \n",
        "        self.timesteps = timesteps\n",
        "        \n",
        "        # Generate the tree with all nodes values = 0\n",
        "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
        "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
        "        # Parent nodes = capacity - 1\n",
        "        # Leaf nodes = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.i = 0\n",
        "        \"\"\" tree:\n",
        "            0\n",
        "           / \\\n",
        "          0   0\n",
        "         / \\ / \\\n",
        "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Contains the experiences (so the size of data is capacity)\n",
        "        self.data = np.zeros((capacity,84*84*2+3), dtype=object)\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
        "    \"\"\"\n",
        "    def add(self, priority, data):\n",
        "        # Look at what index we want to put the experience\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        \n",
        "        \"\"\" tree:\n",
        "            0\n",
        "           / \\\n",
        "          0   0\n",
        "         / \\ / \\\n",
        "tree_index  0 0  0  We fill the leaves from left to right\n",
        "        \"\"\"\n",
        "                \n",
        "        # Update data frame\n",
        "        self.data[self.data_pointer] = data\n",
        "        \n",
        "        # Update the leaf\n",
        "        self.update(tree_index, priority)\n",
        "        \n",
        "        # Add 1 to data_pointer\n",
        "        self.data_pointer += 1\n",
        "        \n",
        "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
        "            self.data_pointer = 0\n",
        "            \n",
        "        \n",
        "            \n",
        "    \"\"\"\n",
        "    Update the leaf priority score and propagate the change through tree\n",
        "    \"\"\"\n",
        "    def update(self, tree_index, priority):\n",
        "        # Change = new priority score - former priority score\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        \n",
        "        # then propagate the change through tree\n",
        "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
        "            \n",
        "            \"\"\"\n",
        "            Here we want to access the line above\n",
        "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
        "            \n",
        "                0\n",
        "               / \\\n",
        "              1   2\n",
        "             / \\ / \\\n",
        "            3  4 5  [6] \n",
        "            \n",
        "            If we are in leaf at index 6, we updated the priority score\n",
        "            We need then to update index 2 node\n",
        "            So tree_index = (tree_index - 1) // 2\n",
        "            tree_index = (6-1)//2\n",
        "            tree_index = 2 (because // round the result)\n",
        "            \"\"\"\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
        "    \"\"\"\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"\n",
        "        Tree structure and array storage:\n",
        "        Tree index:\n",
        "             0         -> storing priority sum\n",
        "            / \\\n",
        "          1     2\n",
        "         / \\   / \\\n",
        "        3   4 5   6    -> storing priority for experiences\n",
        "        Array type for storing:\n",
        "        [0,1,2,3,4,5,6]\n",
        "        \"\"\"\n",
        "        data_w_history = []\n",
        "        parent_index = 0\n",
        "        \n",
        "        while True: # the while loop is faster than the method in the reference code\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            \n",
        "            # If we reach bottom, end the search\n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "            \n",
        "            else: # downward search, always search for a higher priority node\n",
        "                \n",
        "                if v <= self.tree[left_child_index]:\n",
        "                    parent_index = left_child_index\n",
        "                    \n",
        "                else:\n",
        "                    v -= self.tree[left_child_index]\n",
        "                    parent_index = right_child_index\n",
        "            \n",
        "        data_index = leaf_index - self.capacity + 1\n",
        "        \n",
        "        if len(self.data[(data_index - self.timesteps)  : data_index +1]) == self.timesteps  +1 :\n",
        "           \n",
        "            data_w_history.append(self.data[(data_index - self.timesteps) : data_index + 1])\n",
        "            \n",
        "        else :\n",
        "           ref =  self.data[: data_index + 1]\n",
        "           pad = np.zeros_like(self.data[len(ref) : self.timesteps +1])\n",
        "           \n",
        "           \n",
        "           assert len(np.concatenate((pad, self.data[: data_index + 1]))) == self.timesteps + 1\n",
        "           #print(np.shape(np.concatenate((pad, self.data[: data_index + 1]))))\n",
        "           data_w_history.append(np.concatenate((pad, self.data[: data_index + 1]), axis = 0))\n",
        "          \n",
        "        return leaf_index, self.tree[leaf_index], data_w_history\n",
        "    \n",
        "    @property\n",
        "    def total_priority(self):\n",
        "        return self.tree[0] # Returns the root node\n",
        "\n",
        "\n",
        "\n",
        "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    \"\"\"\n",
        "    This Memory class is modified based on the original code from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "    \"\"\"\n",
        "    epsilon = 0.01  # small amount to avoid zero priority\n",
        "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
        "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
        "    beta_increment_per_sampling = 0.01\n",
        "    abs_err_upper = 1.  # clipped abs error\n",
        "\n",
        "    def __init__(self, capacity, timesteps):\n",
        "        self.tree = SumTree(capacity, timesteps)\n",
        "\n",
        "    def store(self, transition):\n",
        "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "        if max_p == 0:\n",
        "            max_p = self.abs_err_upper\n",
        "        self.tree.add(max_p, transition)   # set the max p for new p\n",
        "\n",
        "    def sample(self, n):\n",
        "        \n",
        "        b_idx = []\n",
        "        b_memory = []\n",
        "        ISWeights = []\n",
        "        \n",
        "        #b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n,1)), np.empty((n, 1))\n",
        "        \n",
        "        pri_seg = self.tree.total_priority / n       # priority segment\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "\n",
        "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority     # for later calculate ISweight\n",
        "        for i in range(n):\n",
        "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
        "            v = np.random.uniform(a, b)\n",
        "            idx, p, data = self.tree.get_leaf(v)\n",
        "            prob = p / self.tree.total_priority\n",
        "            ISWeights.append(np.power(prob/(min_prob + 1.e-5), -self.beta))\n",
        "            \n",
        "            b_idx.append(idx)\n",
        "            \n",
        "            b_memory.append(data)\n",
        "        return b_idx, b_memory, ISWeights\n",
        "\n",
        "    def batch_update(self, tree_idx, abs_errors):\n",
        "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
        "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        \n",
        "        for ti, p in zip(tree_idx, ps):\n",
        "            self.tree.update(ti, p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG9Qk0vtA-b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Dec 24 09:56:58 2018\n",
        "\n",
        "@author: maheryatim\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import functools\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "\"\"\"\n",
        "from conv import conv_to_rnn\n",
        "\"\"\"\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "mapping = {}\n",
        "\n",
        "def register(name):\n",
        "    def _thunk(f):\n",
        "        mapping[name] = f\n",
        "        return f\n",
        "    return _thunk\n",
        "\n",
        "\n",
        "def get_network_builder(name):\n",
        "    \"\"\"\n",
        "    If you want to register your own network outside models.py, you just need:\n",
        "    Usage Example:\n",
        "    -------------\n",
        "    from baselines.common.models import register\n",
        "    @register(\"your_network_name\")\n",
        "    def your_network_define(**net_kwargs):\n",
        "        ...\n",
        "        return network_fn\n",
        "    \"\"\"\n",
        "    if name in mapping:\n",
        "        return mapping[name]\n",
        "        \n",
        "    else:\n",
        "        raise ValueError('Unknown network type: {}'.format(name))\n",
        "\n",
        "def huber_loss(x, delta=1.0):\n",
        "    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n",
        "    return tf.where(\n",
        "        tf.abs(x) < delta,\n",
        "        tf.square(x) * 0.5,\n",
        "        delta * (tf.abs(x) - 0.5 * delta)\n",
        "    )\n",
        "\n",
        "       \n",
        "def doublewrap(function):\n",
        "    \"\"\"\n",
        "    A decorator decorator, allowing to use the decorator to be used without\n",
        "    parentheses if no arguments are provided. All arguments must be optional.\n",
        "    \"\"\"\n",
        "    @functools.wraps(function)\n",
        "    def decorator(*args, **kwargs):\n",
        "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
        "            return function(args[0])\n",
        "        else:\n",
        "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@doublewrap\n",
        "def define_scope(function, scope=None, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A decorator for functions that define TensorFlow operations. The wrapped\n",
        "    function will only be executed once. Subsequent calls to it will directly\n",
        "    return the result so that operations are added to the graph only once.\n",
        "    The operations added by the function live within a tf.variable_scope(). If\n",
        "    this decorator is used with arguments, they will be forwarded to the\n",
        "    variable scope. The scope name defaults to the name of the wrapped\n",
        "    function.\n",
        "    \"\"\"\n",
        "    attribute = '_cache_' + function.__name__\n",
        "    name = scope or function.__name__\n",
        "    @property\n",
        "    @functools.wraps(function)\n",
        "    def decorator(self):\n",
        "        if not hasattr(self, attribute):\n",
        "            \n",
        "            with tf.variable_scope(name, *args, **kwargs):\n",
        "                setattr(self, attribute, function(self))\n",
        "        return getattr(self, attribute)\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@register('lstm')\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, inputs, params, seq, batch, action_dim, rewards = None, actions = None, \\\n",
        "                 actions_next = None, q_tar = None, w = None, double = True, dueling = True):\n",
        "        \n",
        "        self.gamma = 0.9\n",
        "        self.actions_dim = action_dim\n",
        "        self.inputs = inputs\n",
        "        self.double = double\n",
        "        self.actions_next = actions_next\n",
        "        self.params = params\n",
        "        self.dueling = dueling \n",
        "        self.seq = seq\n",
        "        self.batch = batch\n",
        "        self.ISWeights = w\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "        self.q_tar = q_tar\n",
        "        self.lr = 1.e-4\n",
        "        self.prediction\n",
        "        self.optimize \n",
        "        self.error\n",
        "        \n",
        "     \n",
        "        \n",
        "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
        "    def prediction(self):\n",
        "        #x = tf.layers.batch_normalization(self.inputs)\n",
        "        \n",
        "        x = conv_to_rnn(self.inputs, self.batch, self.seq)\n",
        "        lstm_layers = [tf.nn.rnn_cell.LSTMCell(\n",
        "                            size,\n",
        "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                            forget_bias=0) for size in self.params]\n",
        "                        \n",
        "        multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_layers)\n",
        "        x, _ = tf.nn.dynamic_rnn(\n",
        "                    cell=multi_rnn_cell, inputs=x, dtype=tf.float32)\n",
        "        \n",
        "        x = tf.layers.batch_normalization(x)\n",
        "        x = tf.reshape(x, (self.batch, self.params[-1]*self.seq))\n",
        "        \n",
        "        \n",
        "        if self.dueling : \n",
        "            advs = tf.contrib.layers.fully_connected(x, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "            advs = tf.contrib.layers.fully_connected(advs, num_outputs=self.actions_dim, activation_fn=None)\n",
        "            state_value = tf.contrib.layers.fully_connected(x, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "            state_value = tf.contrib.layers.fully_connected(state_value, num_outputs=1, activation_fn=None)\n",
        "            action_scores_mean = tf.reduce_mean(advs, 1)\n",
        "            action_scores_centered = advs - tf.expand_dims(action_scores_mean, 1)\n",
        "            q_out = state_value + action_scores_centered\n",
        "            out = q_out\n",
        "\n",
        "        else:\n",
        "            out = tf.contrib.layers.fully_connected(x, num_outputs=self.actions_dim, activation_fn=tf.nn.relu)\n",
        "            out = tf.contrib.layers.fully_connected(out, num_outputs=self.actions_dim, activation_fn=tf.nn.relu)\n",
        "            \n",
        "        \n",
        "        \n",
        "        return out      \n",
        "  \n",
        "    \n",
        "    \n",
        "    @define_scope()\n",
        "    def error(self):\n",
        "                \n",
        "                self.q_eval_wrt_a = tf.reduce_sum(self.prediction * tf.one_hot(self.actions, self.actions_dim), 1)\n",
        "                if self.double : \n",
        "                    actions_index = self.actions_next + tf.range(\n",
        "                                           0, self.batch) * self.q_tar.get_shape().as_list()[1]\n",
        "                    pred_target = tf.gather(tf.reshape(self.q_tar, [-1]), actions_index)\n",
        "                    TD = pred_target*0.9 + self.rewards \n",
        "                else : \n",
        "                    pred_target = self.q_tar\n",
        "                    m = tf.reduce_max(pred_target, axis = 1)\n",
        "                    TD = m *self.gamma + self.rewards \n",
        "                    \n",
        "                TD = tf.stop_gradient(TD)\n",
        "                error = TD - self.q_eval_wrt_a\n",
        "                errors = huber_loss(error)\n",
        "                self.loss = tf.reduce_mean(self.ISWeights * errors)\n",
        "                return  self.loss\n",
        "            \n",
        "    @define_scope()\n",
        "    def abs_error(self):\n",
        "        TD = tf.reduce_max(self.q_tar, axis = 1)*self.gamma + self.rewards \n",
        "        self.abs_errors = tf.abs(TD-self.q_eval_wrt_a)\n",
        "        return self.abs_errors\n",
        "        \n",
        "    @define_scope()\n",
        "    def optimize(self):\n",
        "                \"\"\"\n",
        "                grads_and_vars=tf.train.AdamOptimizer(self.lr).compute_gradients(self.error)\n",
        "                for g, v in grads_and_vars:\n",
        "                    if g is not None:\n",
        "                        #print(format(v.name))\n",
        "                        tf.summary.histogram(\"{}/grad_histogram\".format(v.name), g)\n",
        "                        tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                \n",
        "                \"\"\"  \n",
        "                self._train_op = tf.train.AdamOptimizer(self.lr).minimize(self.error)\n",
        "                return self._train_op\n",
        "\n",
        "\n",
        "    \n",
        "@register('lstm_target')\n",
        "class Model_:\n",
        "\n",
        "    def __init__(self, inputs, params, seq, batch, actions_dim, dueling = True):\n",
        "        \n",
        "        self.dueling = dueling \n",
        "        self.inputs = inputs\n",
        "        self.params = params\n",
        "        self.seq = seq\n",
        "        self.batch = batch\n",
        "        self.actions_dim = actions_dim\n",
        "        self.prediction\n",
        "        \n",
        "    \n",
        "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
        "    def prediction(self):\n",
        "        #x = tf.layers.batch_normalization(self.inputs)      \n",
        "        x = conv_to_rnn(self.inputs, self.batch, self.seq)\n",
        "        lstm_layers = [tf.nn.rnn_cell.LSTMCell(\n",
        "                            size,\n",
        "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                            forget_bias=0) for size in self.params]\n",
        "                        \n",
        "        multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_layers)\n",
        "        x, _ = tf.nn.dynamic_rnn(\n",
        "                    cell=multi_rnn_cell, inputs=x, dtype=tf.float32)\n",
        "        x = tf.layers.batch_normalization(x)\n",
        "        \n",
        "        x = tf.reshape(x, (self.batch, self.params[-1]*self.seq))\n",
        "        \n",
        "        \n",
        "        if self.dueling : \n",
        "            advs = tf.contrib.layers.fully_connected(x, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "            advs = tf.contrib.layers.fully_connected(advs, num_outputs=self.actions_dim, activation_fn=None)\n",
        "            state_value = tf.contrib.layers.fully_connected(x, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "            state_value = tf.contrib.layers.fully_connected(state_value, num_outputs=1, activation_fn=None)\n",
        "            action_scores_mean = tf.reduce_mean(advs, 1)\n",
        "            action_scores_centered = advs - tf.expand_dims(action_scores_mean, 1)\n",
        "            q_out = state_value + action_scores_centered\n",
        "            out = q_out\n",
        "\n",
        "        else:\n",
        "            out = tf.contrib.layers.fully_connected(x, num_outputs=self.actions_dim, activation_fn=tf.nn.relu)\n",
        "            out = tf.contrib.layers.fully_connected(out, num_outputs=self.actions_dim, activation_fn=tf.nn.relu)\n",
        "            \n",
        "        \n",
        "        return out      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5SIsQLdBJXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Dec 24 11:01:53 2018\n",
        "\n",
        "@author: maheryatim\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "import numpy as np \n",
        "#from utils import get_session \n",
        "\n",
        "\n",
        "\n",
        "def perturb(original_scope, perturbed_scope):\n",
        "\n",
        "    filter_vars = ['fully_connected_1', 'fully_connected_2']\n",
        "    original_params = []\n",
        "    original_params_per = []\n",
        "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=original_scope):\n",
        "        for filter_var in filter_vars:\n",
        "            if filter_var in var.name:\n",
        "                original_params_per.append(var)\n",
        "            else : \n",
        "                 original_params.append(var)\n",
        "    perturbed_params = []\n",
        "    perturbed_params_per = []\n",
        "\n",
        "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=original_scope):\n",
        "            for filter_var in filter_vars:\n",
        "                if filter_var in var.name:\n",
        "                    perturbed_params_per.append(var)\n",
        "                else : \n",
        "                    perturbed_params.append(var)\n",
        "\n",
        "    return original_params, original_params_per, perturbed_params, perturbed_params_per\n",
        "\n",
        "\n",
        "def adapt(original_scope, perturbed_scope):\n",
        "\n",
        "    filter_vars = ['fully_connected_1', 'fully_connected_2']\n",
        "    original_params = []\n",
        "    original_params_per = []\n",
        "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=original_scope):\n",
        "        for filter_var in filter_vars:\n",
        "            if filter_var in var.name:\n",
        "                original_params_per.append(var)\n",
        "            else : \n",
        "                 original_params.append(var)\n",
        "    perturbed_params = []\n",
        "    perturbed_params_per = []\n",
        "\n",
        "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=original_scope):\n",
        "            for filter_var in filter_vars:\n",
        "                if filter_var in var.name:\n",
        "                    perturbed_params_per.append(var)\n",
        "                else : \n",
        "                    perturbed_params.append(var)\n",
        "\n",
        "    return original_params, original_params_per, perturbed_params, perturbed_params_per\n",
        "\n",
        "\n",
        "\n",
        "def change(original_scope, perturbed_scope, param_noise_scale) : \n",
        "    \n",
        "    if perturbed_scope == 'adaptive_model' : \n",
        "       original_params, original_params_per, perturbed_params, perturbed_params_per\\\n",
        "       = adapt(original_scope, perturbed_scope)\n",
        "    elif perturbed_scope == 'perturbed_model' : \n",
        "        original_params, original_params_per, perturbed_params, perturbed_params_per\\\n",
        "        = perturb(original_scope, perturbed_scope)\n",
        "\n",
        "               \n",
        "    for var, perturbed_var in zip(original_params, perturbed_params):\n",
        "                    tf.assign(perturbed_var, var)\n",
        "    for var, perturbed_var in zip(original_params_per, perturbed_params_per):\n",
        "                    tf.assign(perturbed_var, var + tf.random_normal(shape=tf.shape(var), mean=0., stddev=param_noise_scale))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "def doublewrap(function):\n",
        "    \"\"\"\n",
        "    A decorator decorator, allowing to use the decorator to be used without\n",
        "    parentheses if no arguments are provided. All arguments must be optional.\n",
        "    \"\"\"\n",
        "    @functools.wraps(function)\n",
        "    def decorator(*args, **kwargs):\n",
        "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
        "            return function(args[0])\n",
        "        else:\n",
        "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@doublewrap\n",
        "def define_scope(function, scope=None, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A decorator for functions that define TensorFlow operations. The wrapped\n",
        "    function will only be executed once. Subsequent calls to it will directly\n",
        "    return the result so that operations are added to the graph only once.\n",
        "    The operations added by the function live within a tf.variable_scope(). If\n",
        "    this decorator is used with arguments, they will be forwarded to the\n",
        "    variable scope. The scope name defaults to the name of the wrapped\n",
        "    function.\n",
        "    \"\"\"\n",
        "    attribute = '_cache_' + function.__name__\n",
        "    name = scope or function.__name__\n",
        "    @property\n",
        "    @functools.wraps(function)\n",
        "    def decorator(self):\n",
        "        if not hasattr(self, attribute):\n",
        "            \n",
        "            with tf.variable_scope(name, *args, **kwargs):\n",
        "                setattr(self, attribute, function(self))\n",
        "        return getattr(self, attribute)\n",
        "    return decorator\n",
        "\n",
        "\n",
        "class noisy_actions : \n",
        "    \n",
        "    def __init__(self, params, obs, step, delta, eps_state, q_value,q_values_adaptive,q_values_perturbed):\n",
        "        \n",
        "        self.timesteps_rnn = tf.placeholder(tf.int32, shape=(None), name='shape')\n",
        "        self.batch_ = tf.placeholder(tf.int32, shape=(None), name='shape')\n",
        "        self.states = tf.placeholder(tf.float32, shape=[None,84,84,1])\n",
        "        self.sess = get_session()\n",
        "        self.obs = obs\n",
        "        self.step = step           \n",
        "        self.delta = delta\n",
        "        self.eps_state = eps_state\n",
        "        self.q_values = q_value\n",
        "        self.q_values_adaptive = q_values_adaptive\n",
        "        self.q_values_perturbed = q_values_perturbed\n",
        "        self.act_\n",
        "\n",
        "        \n",
        "    @define_scope\n",
        "    def act_(self): \n",
        "        with tf.variable_scope(\"var\", reuse=tf.AUTO_REUSE):\n",
        "            variance = tf.get_variable(\"variance\", (), initializer=tf.constant_initializer(2.19), trainable=False)\n",
        "        \n",
        "            \n",
        "        #once epsilon is 0.01, stop updating params, perturbed = original barely \n",
        "        \n",
        "        if self.step == 0 : \n",
        "            change('Q_value','perturbed_model', variance)\n",
        "        \n",
        "        if self.step % 100 == 0 and self.step > 0 : \n",
        "            change('Q_value','adaptive_model', variance)\n",
        "            \n",
        "            kl = tf.reduce_sum(tf.nn.sigmoid(self.q_values) * (tf.log(tf.nn.sigmoid(self.q_values))\\\n",
        "                                                 - tf.log(tf.nn.sigmoid(self.q_values_adaptive))),\\\n",
        "                           axis=-1)\n",
        "            mean_kl = tf.reduce_mean(kl)\n",
        "                \n",
        "            if mean_kl < self.delta : \n",
        "                variance.assign(variance * 1.01)\n",
        "            else : variance.assign(variance / 1.01)\n",
        "        \n",
        "        \n",
        "        random_action = np.random.randint(0, 9)\n",
        "        if np.random.uniform() < self.eps_state:\n",
        "                action = random_action \n",
        "        else : action = tf.argmax(self.q_values_perturbed, axis=1)\n",
        "                \n",
        "        return action\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgW-qOoCBNWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Dec 24 11:00:42 2018\n",
        "\n",
        "@author: maheryatim\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import os  \n",
        "import multiprocessing \n",
        "import joblib\n",
        "\n",
        "\n",
        "def get_s(): \n",
        "    return tf.Session()\n",
        "\n",
        "def get_session(config=None):\n",
        "    \"\"\"Get default session or create one with a given config\"\"\"\n",
        "    \n",
        "    sess = tf.get_default_session()\n",
        "   \n",
        "    if sess is None:\n",
        "        #tf.reset_default_graph()\n",
        "        \n",
        "        sess = make_session(config=config, make_default=True)\n",
        "    return sess\n",
        "\n",
        "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n",
        "    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n",
        "    if num_cpu is None:\n",
        "        num_cpu = int(os.getenv('RCALL_NUM_CPU', multiprocessing.cpu_count()))\n",
        "    if config is None:\n",
        "        config = tf.ConfigProto(\n",
        "            allow_soft_placement=True,\n",
        "            inter_op_parallelism_threads=num_cpu,\n",
        "            intra_op_parallelism_threads=num_cpu)\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "    if make_default:\n",
        "        return tf.InteractiveSession(config=config, graph=graph)\n",
        "    else:\n",
        "        return tf.Session(config=config, graph=graph)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ALREADY_INITIALIZED = set()\n",
        "\n",
        "def initialize():\n",
        "    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\n",
        "    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n",
        "    get_session().run(tf.variables_initializer(new_variables))\n",
        "    ALREADY_INITIALIZED.update(new_variables)\n",
        "    \n",
        "    \n",
        "    \n",
        "def save_variables(save_path, variables=None, sess=None):\n",
        "    sess = sess or get_session()\n",
        "    variables = variables or tf.trainable_variables()\n",
        "\n",
        "    ps = sess.run(variables)\n",
        "    save_dict = {v.name: value for v, value in zip(variables, ps)}\n",
        "    dirname = os.path.dirname(save_path)\n",
        "    if any(dirname):\n",
        "        os.makedirs(dirname, exist_ok=True)\n",
        "    joblib.dump(save_dict, save_path)\n",
        "\n",
        "def load_variables(load_path, variables=None, sess=None):\n",
        "    sess = sess or get_session()\n",
        "    variables = variables or tf.trainable_variables()\n",
        "\n",
        "    loaded_params = joblib.load(os.path.expanduser(load_path))\n",
        "    restores = []\n",
        "    if isinstance(loaded_params, list):\n",
        "        assert len(loaded_params) == len(variables), 'number of variables loaded mismatches len(variables)'\n",
        "        for d, v in zip(loaded_params, variables):\n",
        "            restores.append(v.assign(d))\n",
        "    else:\n",
        "        for v in variables:\n",
        "            restores.append(v.assign(loaded_params[v.name]))\n",
        "\n",
        "    sess.run(restores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEjceEBhBRAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "\"\"\"\n",
        "from memory import Memory\n",
        "from model import Model_, get_network_builder\n",
        "from noisy_action import noisy_actions \n",
        "from utils import get_session, initialize, save_variables\n",
        "\"\"\"\n",
        "import os\n",
        "import logging \n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "\n",
        "\n",
        "\n",
        "def process_observation(observation):\n",
        "    assert observation.ndim == 3\n",
        "    img = Image.fromarray(observation)\n",
        "    img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
        "    processed_observation = np.array(img)\n",
        "    assert processed_observation.shape == INPUT_SHAPE\n",
        "    return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "def process_state_batch(batch):\n",
        "    processed_batch = batch.astype('float32') / 255.\n",
        "    return processed_batch\n",
        "\n",
        "def process_reward(reward):\n",
        "    return np.clip(reward, -1., 1.)\n",
        "act_dim = 9\n",
        "\n",
        "class RDeepRL():\n",
        "\n",
        "    def __init__(self, env,\n",
        "                 model_type,\n",
        "                 seed=None,\n",
        "                 lr=5e-4,\n",
        "                 history_size=2,\n",
        "                 memory_size=5000,\n",
        "                 exploration_fraction=0.1,\n",
        "                 exploration_final_eps=0.02,\n",
        "                 batch_size=100,\n",
        "                 save_model_path=None,\n",
        "                 steps_before_learning=1000,\n",
        "                 gamma=0.9,\n",
        "                 target_network_update_freq=1000,\n",
        "                 load_path=None, \n",
        "                 double = False,\n",
        "                 **network_kwargs):\n",
        "        self.memory_size = memory_size\n",
        "        self.double = double\n",
        "        self.history_size = history_size\n",
        "        self.memory = Memory(memory_size, self.history_size)\n",
        "        self.model_type = model_type\n",
        "        self.model_params = network_kwargs\n",
        "        self.shape =  [84,84,1] \n",
        "        \n",
        "        \n",
        "        tf.reset_default_graph()\n",
        "\n",
        "    def _build_graph(self):\n",
        "\n",
        "        if self.model_type == 'lstm':\n",
        "            model_Q = get_network_builder('lstm')\n",
        "            model_target = get_network_builder('lstm_target')\n",
        "            self.net_params = {\n",
        "                \n",
        "                'size': self.model_params.get('lstm_size', [256]),\n",
        "                \n",
        "            }\n",
        "\n",
        "        elif self.model_type == 'rhh':\n",
        "            model_Q = get_network_builder('lstm')\n",
        "            model_target = get_network_builder('lstm_target')\n",
        "            self.net_params = {\n",
        "                \"\"\"IMPLEMENT\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "        elif self.model_type == 'tensor':\n",
        "            model_Q = get_network_builder('lstm')\n",
        "            model_target = get_network_builder('lstm_target')\n",
        "            self.net_params = {\n",
        "                \"\"\"IMPLEMENT\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Unknown model type: '%s'\" %\n",
        "                self.model_type)\n",
        "\n",
        "        self.timesteps_rnn = tf.placeholder(tf.int32, shape=(None), name='shape')\n",
        "        self.batch_ = tf.placeholder(tf.int32, shape=(None), name='shape')\n",
        "        \n",
        "        with tf.variable_scope('Q_value', reuse = tf.AUTO_REUSE):\n",
        "            self.states = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape=[\n",
        "                    None,\n",
        "                    self.shape[0],\n",
        "                    self.shape[1],\n",
        "                    self.shape[2]])\n",
        "            self.actions_next = tf.placeholder(tf.int32, shape=(None), name='action_next')\n",
        "            self.actions = tf.placeholder(tf.int32, shape=(None,), name='action')\n",
        "            self.rewards = tf.placeholder(tf.float32, shape=(None,), name='reward')\n",
        "            self.q_tar = tf.placeholder(\n",
        "                tf.float32, [\n",
        "                    None, env.action_space.n], name='Q_target')\n",
        "            self.up_weights = tf.placeholder(\n",
        "                tf.float32, shape=(\n",
        "                    None,), name='priority_weights')\n",
        "            self.model_Q = model_Q(\n",
        "                self.states, self.net_params['size'], self.history_size +1, self.batch_, env.action_space.n,\n",
        "                                            \n",
        "                                             self.rewards,\n",
        "                                             self.actions,\n",
        "                                             self.actions_next,\n",
        "                                             self.q_tar,\n",
        "                                             self.up_weights)\n",
        "\n",
        "\n",
        "        with tf.variable_scope('Target', reuse = tf.AUTO_REUSE):\n",
        "            \n",
        "            self.states_next = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape=[\n",
        "                    None,\n",
        "                    self.shape[0],\n",
        "                    self.shape[1],\n",
        "                    self.shape[2]])\n",
        "            \n",
        "             \n",
        "            self.model_target = model_target(self.states_next,\n",
        "                                             self.net_params['size'],\n",
        "                                             self.history_size +1,\n",
        "                                             self.batch_,\n",
        "                                             env.action_space.n,\n",
        "                                             )\n",
        "        \n",
        "        for var in tf.trainable_variables():\n",
        "            tf.summary.histogram(var.name, var) \n",
        "        \n",
        "        \n",
        "        self.obs = tf.placeholder(tf.float32,shape=[None,84,84,1])\n",
        "        self.step = tf.placeholder(tf.int32)             \n",
        "        self.delta = tf.placeholder(tf.float32)\n",
        "        \n",
        "        self.q_value = tf.placeholder(tf.float32, [None, act_dim ])\n",
        "        self.q_values_adaptive = tf.placeholder(tf.float32, [None, act_dim ])\n",
        "        self.q_values_perturbed = tf.placeholder(tf.float32, [None, act_dim ])\n",
        "        with tf.variable_scope('perturbed_model'):\n",
        "          \n",
        "            self.m_perturbed = Model_(self.states, self.net_params['size'], self.history_size +1, self.batch_ , act_dim )\n",
        "            \n",
        "        with tf.variable_scope('adaptive_model'):\n",
        "          \n",
        "            self.m_adapt = Model_(self.states,self.net_params['size'], self.history_size +1, self.batch_ , act_dim )\n",
        "        \n",
        "        self.action_ = noisy_actions(self.net_params['size'],self.obs, self.step,  self.delta, 0.01,\n",
        "                                     self.q_value, self.q_values_adaptive,self.q_values_perturbed)\n",
        "        self.eps_rand = 0.1\n",
        "        \n",
        "     \n",
        "        self.sess = get_session()\n",
        "        initialize()\n",
        "     \n",
        "        \n",
        "    def update_target_weights(self, scope_q, scope_target):\n",
        "\n",
        "        with tf.variable_scope('soft_replacement'):\n",
        "           \n",
        "        \n",
        "            e_params = tf.trainable_variables(\n",
        "                    \n",
        "                    scope=scope_target)\n",
        "            t_params = tf.trainable_variables(\n",
        "                    scope=scope_q)[\n",
        "                    :len(e_params)]\n",
        "        \n",
        "        for i in range(len(e_params)):\n",
        "            self.sess.run(e_params[i].assign(tf.multiply(t_params[i], 0.95) \\\n",
        "                                                    + tf.multiply(e_params[i], 1 - 0.95)))\n",
        "                \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    def act(self, obs_, step, epsilon, noisy_):\n",
        "     \n",
        "        if step < self.memory_size : \n",
        "            action = np.random.randint(0, act_dim)\n",
        "            return action\n",
        "        \n",
        "        obs_ = np.reshape(obs_, (-1, self.shape[0], self.shape[1], self.shape[2]))\n",
        "        actions_values = self.sess.run(self.model_Q.prediction, feed_dict={self.states: obs_, self.batch_ : 1\n",
        "                                                                          })\n",
        "        \n",
        "        if noisy_ == False : \n",
        "            random_action = np.random.randint(0, act_dim)\n",
        "            if np.random.uniform() < epsilon:\n",
        "                action = random_action \n",
        "            else : action =  np.argmax(actions_values, axis=1)[0]\n",
        "          \n",
        "        elif noisy_ == True and epsilon >= 0.01 : \n",
        "            delta = -np.log(1 - epsilon + epsilon/\n",
        "                        env.action_space.n)\n",
        "            q_values_adaptive = self.sess.run(self.m_adapt.prediction, feed_dict={self.states: obs_, self.batch_ : 1})  \n",
        "            q_values_perturbed = self.sess.run(self.m_perturbed.prediction, feed_dict={self.states: obs_, self.batch_ : 1\n",
        "                                                                              })  \n",
        "            action = self.sess.run(self.action_.act_, feed_dict={self.obs:obs_,\n",
        "                                                            self.step:step,           \n",
        "                                                            self.delta:delta,\n",
        "                                                            self.q_value:actions_values,\n",
        "                                                            self.q_values_adaptive:q_values_adaptive,\n",
        "                                                            self.q_values_perturbed:q_values_perturbed,\n",
        "                                                            })\n",
        "        \n",
        "        else : \n",
        "            action =  np.argmax(actions_values, axis=1)[0]\n",
        "        \n",
        "        return action \n",
        "\n",
        "    def build_graph(self): \n",
        "        self._build_graph()\n",
        "    \n",
        "    \n",
        "    @property\n",
        "    def get_sess(self):\n",
        "        return self.sess\n",
        "    \n",
        "    def train(self,  num_episodes, history_size, batch_size, epsilon = 1, render=False, noisy = False):\n",
        "        \n",
        "        \n",
        "        logging.info('\\nTraining starts\\n')\n",
        "        total_steps = 0\n",
        "        \n",
        "        r = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            observation = env.reset()\n",
        "            observation = process_observation(observation)\n",
        "            observation = process_state_batch(observation)\n",
        "            rewards = 0\n",
        "            \n",
        "            recent_history = []\n",
        "            while True:\n",
        "               \n",
        "                if render:\n",
        "                    env.render()\n",
        "                \n",
        "               \n",
        "                recent_history.append(observation)\n",
        "                \n",
        "                if len(recent_history) == history_size + 1:\n",
        "                    action = self.act(recent_history, total_steps, epsilon, noisy)\n",
        "                    recent_history.pop(0)\n",
        "                    \n",
        "                else : \n",
        "                  a = np.zeros_like(observation) \n",
        "                  for i in range(history_size +1 - len(recent_history)) :\n",
        "                    recent_history.insert(0, a)\n",
        "                    \n",
        "                  action = self.act(recent_history, total_steps, epsilon, noisy)\n",
        "                  recent_history.pop(0)\n",
        "                                      \n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                next_state = process_observation(next_state)\n",
        "                next_state = process_state_batch(next_state)\n",
        "                #reward = process_reward(reward)\n",
        "                \n",
        "                df = np.hstack((observation.reshape(84*84,), action, reward, done, next_state.reshape(84*84,)))\n",
        "                self.memory.store(df)\n",
        "                \n",
        "                if done:\n",
        "                    print('\\nEnd of episode {}, rewards last episode : {:.2f} '.format(episode, rewards))\n",
        "                    r.append(rewards)\n",
        "                    break\n",
        "\n",
        "                observation = next_state\n",
        "                rewards += reward\n",
        "                total_steps += 1\n",
        "               \n",
        "                    \n",
        "                if total_steps > self.memory_size and total_steps % 20 == 0  :\n",
        "                   \n",
        "                   self.learn(batch_size=batch_size)\n",
        "                   \n",
        "                if total_steps % 1000 == 0 and total_steps > self.memory_size:  \n",
        "                    logging.info('Epsilon {:.3f}'.format(epsilon))\n",
        "                    epsilon -= 0.0065\n",
        "                    print(\"Epsilon\", epsilon)\n",
        "                    epsilon = max(0.01, epsilon)\n",
        "                if total_steps % 3000 == 0 and total_steps >  self.memory_size:\n",
        "                    self.update_target_weights('Target', 'Q_value')\n",
        "                    print('target params replaced')\n",
        "                if total_steps % 300000 == 0 : \n",
        "                    save_variables('v')\n",
        "                \n",
        "                \n",
        "                \n",
        "            \n",
        "        if render:\n",
        "            env.close()\n",
        "        return r, epsilon\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def learn(self, batch_size=32):\n",
        "        \n",
        "        tree_idx, batch_memory, up_weights_ = self.memory.sample(batch_size)\n",
        "        \n",
        "        \n",
        "        df = np.hstack(batch_memory)\n",
        "        df = np.squeeze(df, axis = 0)\n",
        "        \n",
        "        states = df[:,:84*84]\n",
        "        rewards = df[:, 84*84].reshape(-1,self.history_size + 1)[:,-1]\n",
        "        actions = df[:, 84*84 +1].reshape(-1,self.history_size + 1)[:,-1]\n",
        "        states_next = df[:,84*84 + 3:]\n",
        "        \n",
        "        states = np.reshape(states, (-1,84,84,1))\n",
        "        states_next = np.reshape(states_next, (-1,84,84,1))\n",
        "        self.merged_summary_op = tf.summary.merge_all()\n",
        "        logs_path = os.path.expanduser('tb')\n",
        "        self.summary_writer = tf.summary.FileWriter(logs_path,\n",
        "                                            graph=tf.get_default_graph(),max_queue=20,\n",
        "                                            flush_secs=20)\n",
        "        \n",
        "        q_tar_= self.sess.run(self.model_target.prediction, feed_dict={\n",
        "                            self.states_next: states_next, self.batch_: batch_size})\n",
        "        \n",
        "        summary, actions_next_value = self.sess.run([self.merged_summary_op,self.model_Q.prediction], feed_dict={self.states: states_next, self.states_next: states_next,   self.batch_ :batch_size})\n",
        "        self.summary_writer.add_summary(summary)   \n",
        "        action_next = np.argmax(actions_next_value[:], axis =1)[0]\n",
        "        \n",
        "        \n",
        "        _, cost, abs_errors = self.sess.run([self.model_Q.optimize, self.model_Q.error,\n",
        "                                         self.model_Q.abs_error],\n",
        "                                        feed_dict={self.states: states,\n",
        "                                                   self.rewards: rewards,\n",
        "                                                   self.actions: actions,\n",
        "                                                   self.actions_next : action_next, \n",
        "                                                   self.up_weights: up_weights_,\n",
        "                                                   self.q_tar: q_tar_,\n",
        "                                                   self.batch_: batch_size})\n",
        "\n",
        "        \n",
        "        \n",
        "        self.memory.batch_update(tree_idx, abs_errors)\n",
        "        \n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOCffU4gBqxi",
        "colab_type": "code",
        "outputId": "dd12161a-1093-4b70-a2f2-a6c31edeffe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5117
        }
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
        "tf.logging.set_verbosity(tf.logging.WARN)\n",
        "if __name__ == '__main__':\n",
        "    import gym\n",
        "    \n",
        "    env = gym.make('MsPacman-v0')\n",
        "    if tf.get_default_session() != None : \n",
        "           tf.get_default_session().close()\n",
        "           tf.reset_default_graph() \n",
        "    try :              \n",
        "        rl_model = RDeepRL(env, 'lstm')\n",
        "        rl_model.build_graph()\n",
        "        r_ = rl_model.train(100000,batch_size=128, history_size=2, noisy = True)\n",
        "        \n",
        "        save_variables('v_final')\n",
        "    except KeyboardInterrupt:\n",
        "        tf.get_default_session().close()\n",
        "        print('Train() Interrupted')\n",
        "        rl_model.get_sess.close()\n",
        "        env.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 9, 9, 64]\n",
            "[None, 9, 9, 64]\n",
            "[None, 9, 9, 64]\n",
            "[None, 9, 9, 64]\n",
            "\n",
            "End of episode 0, rewards last episode : 220.00 \n",
            "\n",
            "End of episode 1, rewards last episode : 300.00 \n",
            "\n",
            "End of episode 2, rewards last episode : 140.00 \n",
            "\n",
            "End of episode 3, rewards last episode : 320.00 \n",
            "\n",
            "End of episode 4, rewards last episode : 110.00 \n",
            "\n",
            "End of episode 5, rewards last episode : 260.00 \n",
            "\n",
            "End of episode 6, rewards last episode : 200.00 \n",
            "\n",
            "End of episode 7, rewards last episode : 230.00 \n",
            "\n",
            "End of episode 8, rewards last episode : 400.00 \n",
            "Epsilon 0.9935\n",
            "target params replaced\n",
            "\n",
            "End of episode 9, rewards last episode : 2080.00 \n",
            "Epsilon 0.9870000000000001\n",
            "\n",
            "End of episode 10, rewards last episode : 320.00 \n",
            "Epsilon 0.9805000000000001\n",
            "\n",
            "End of episode 11, rewards last episode : 510.00 \n",
            "\n",
            "End of episode 12, rewards last episode : 250.00 \n",
            "Epsilon 0.9740000000000002\n",
            "target params replaced\n",
            "\n",
            "End of episode 13, rewards last episode : 320.00 \n",
            "\n",
            "End of episode 14, rewards last episode : 520.00 \n",
            "Epsilon 0.9675000000000002\n",
            "\n",
            "End of episode 15, rewards last episode : 870.00 \n",
            "Epsilon 0.9610000000000003\n",
            "\n",
            "End of episode 16, rewards last episode : 360.00 \n",
            "Epsilon 0.9545000000000003\n",
            "target params replaced\n",
            "\n",
            "End of episode 17, rewards last episode : 630.00 \n",
            "Epsilon 0.9480000000000004\n",
            "\n",
            "End of episode 18, rewards last episode : 470.00 \n",
            "Epsilon 0.9415000000000004\n",
            "\n",
            "End of episode 19, rewards last episode : 1920.00 \n",
            "\n",
            "End of episode 20, rewards last episode : 240.00 \n",
            "Epsilon 0.9350000000000005\n",
            "target params replaced\n",
            "\n",
            "End of episode 21, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 22, rewards last episode : 520.00 \n",
            "Epsilon 0.9285000000000005\n",
            "\n",
            "End of episode 23, rewards last episode : 900.00 \n",
            "Epsilon 0.9220000000000006\n",
            "\n",
            "End of episode 24, rewards last episode : 520.00 \n",
            "\n",
            "End of episode 25, rewards last episode : 250.00 \n",
            "Epsilon 0.9155000000000006\n",
            "target params replaced\n",
            "\n",
            "End of episode 26, rewards last episode : 370.00 \n",
            "Epsilon 0.9090000000000007\n",
            "\n",
            "End of episode 27, rewards last episode : 1720.00 \n",
            "Epsilon 0.9025000000000007\n",
            "\n",
            "End of episode 28, rewards last episode : 200.00 \n",
            "\n",
            "End of episode 29, rewards last episode : 200.00 \n",
            "Epsilon 0.8960000000000008\n",
            "target params replaced\n",
            "\n",
            "End of episode 30, rewards last episode : 470.00 \n",
            "Epsilon 0.8895000000000008\n",
            "\n",
            "End of episode 31, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 32, rewards last episode : 520.00 \n",
            "Epsilon 0.8830000000000009\n",
            "\n",
            "End of episode 33, rewards last episode : 370.00 \n",
            "Epsilon 0.876500000000001\n",
            "target params replaced\n",
            "\n",
            "End of episode 34, rewards last episode : 1070.00 \n",
            "Epsilon 0.870000000000001\n",
            "\n",
            "End of episode 35, rewards last episode : 470.00 \n",
            "\n",
            "End of episode 36, rewards last episode : 520.00 \n",
            "Epsilon 0.863500000000001\n",
            "\n",
            "End of episode 37, rewards last episode : 230.00 \n",
            "\n",
            "End of episode 38, rewards last episode : 230.00 \n",
            "Epsilon 0.8570000000000011\n",
            "target params replaced\n",
            "\n",
            "End of episode 39, rewards last episode : 520.00 \n",
            "\n",
            "End of episode 40, rewards last episode : 510.00 \n",
            "Epsilon 0.8505000000000011\n",
            "\n",
            "End of episode 41, rewards last episode : 200.00 \n",
            "Epsilon 0.8440000000000012\n",
            "\n",
            "End of episode 42, rewards last episode : 670.00 \n",
            "Epsilon 0.8375000000000012\n",
            "target params replaced\n",
            "\n",
            "End of episode 43, rewards last episode : 670.00 \n",
            "Epsilon 0.8310000000000013\n",
            "\n",
            "End of episode 44, rewards last episode : 2080.00 \n",
            "Epsilon 0.8245000000000013\n",
            "\n",
            "End of episode 45, rewards last episode : 320.00 \n",
            "Epsilon 0.8180000000000014\n",
            "target params replaced\n",
            "\n",
            "End of episode 46, rewards last episode : 490.00 \n",
            "\n",
            "End of episode 47, rewards last episode : 240.00 \n",
            "Epsilon 0.8115000000000014\n",
            "\n",
            "End of episode 48, rewards last episode : 320.00 \n",
            "\n",
            "End of episode 49, rewards last episode : 500.00 \n",
            "Epsilon 0.8050000000000015\n",
            "\n",
            "End of episode 50, rewards last episode : 490.00 \n",
            "Epsilon 0.7985000000000015\n",
            "target params replaced\n",
            "\n",
            "End of episode 51, rewards last episode : 570.00 \n",
            "Epsilon 0.7920000000000016\n",
            "\n",
            "End of episode 52, rewards last episode : 670.00 \n",
            "\n",
            "End of episode 53, rewards last episode : 490.00 \n",
            "Epsilon 0.7855000000000016\n",
            "\n",
            "End of episode 54, rewards last episode : 1770.00 \n",
            "Epsilon 0.7790000000000017\n",
            "target params replaced\n",
            "\n",
            "End of episode 55, rewards last episode : 220.00 \n",
            "\n",
            "End of episode 56, rewards last episode : 330.00 \n",
            "Epsilon 0.7725000000000017\n",
            "\n",
            "End of episode 57, rewards last episode : 1070.00 \n",
            "Epsilon 0.7660000000000018\n",
            "\n",
            "End of episode 58, rewards last episode : 380.00 \n",
            "\n",
            "End of episode 59, rewards last episode : 530.00 \n",
            "Epsilon 0.7595000000000018\n",
            "target params replaced\n",
            "\n",
            "End of episode 60, rewards last episode : 180.00 \n",
            "\n",
            "End of episode 61, rewards last episode : 200.00 \n",
            "Epsilon 0.7530000000000019\n",
            "\n",
            "End of episode 62, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 63, rewards last episode : 240.00 \n",
            "Epsilon 0.7465000000000019\n",
            "\n",
            "End of episode 64, rewards last episode : 500.00 \n",
            "Epsilon 0.740000000000002\n",
            "target params replaced\n",
            "\n",
            "End of episode 65, rewards last episode : 520.00 \n",
            "\n",
            "End of episode 66, rewards last episode : 660.00 \n",
            "Epsilon 0.733500000000002\n",
            "\n",
            "End of episode 67, rewards last episode : 2470.00 \n",
            "Epsilon 0.7270000000000021\n",
            "\n",
            "End of episode 68, rewards last episode : 380.00 \n",
            "\n",
            "End of episode 69, rewards last episode : 260.00 \n",
            "Epsilon 0.7205000000000021\n",
            "target params replaced\n",
            "\n",
            "End of episode 70, rewards last episode : 320.00 \n",
            "\n",
            "End of episode 71, rewards last episode : 200.00 \n",
            "Epsilon 0.7140000000000022\n",
            "\n",
            "End of episode 72, rewards last episode : 420.00 \n",
            "Epsilon 0.7075000000000022\n",
            "\n",
            "End of episode 73, rewards last episode : 510.00 \n",
            "Epsilon 0.7010000000000023\n",
            "target params replaced\n",
            "\n",
            "End of episode 74, rewards last episode : 870.00 \n",
            "Epsilon 0.6945000000000023\n",
            "\n",
            "End of episode 75, rewards last episode : 370.00 \n",
            "\n",
            "End of episode 76, rewards last episode : 560.00 \n",
            "Epsilon 0.6880000000000024\n",
            "\n",
            "End of episode 77, rewards last episode : 520.00 \n",
            "\n",
            "End of episode 78, rewards last episode : 300.00 \n",
            "Epsilon 0.6815000000000024\n",
            "target params replaced\n",
            "\n",
            "End of episode 79, rewards last episode : 400.00 \n",
            "Epsilon 0.6750000000000025\n",
            "\n",
            "End of episode 80, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 81, rewards last episode : 500.00 \n",
            "Epsilon 0.6685000000000025\n",
            "\n",
            "End of episode 82, rewards last episode : 670.00 \n",
            "Epsilon 0.6620000000000026\n",
            "target params replaced\n",
            "\n",
            "End of episode 83, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 84, rewards last episode : 440.00 \n",
            "Epsilon 0.6555000000000026\n",
            "\n",
            "End of episode 85, rewards last episode : 290.00 \n",
            "\n",
            "End of episode 86, rewards last episode : 200.00 \n",
            "Epsilon 0.6490000000000027\n",
            "\n",
            "End of episode 87, rewards last episode : 520.00 \n",
            "Epsilon 0.6425000000000027\n",
            "target params replaced\n",
            "\n",
            "End of episode 88, rewards last episode : 470.00 \n",
            "\n",
            "End of episode 89, rewards last episode : 200.00 \n",
            "Epsilon 0.6360000000000028\n",
            "\n",
            "End of episode 90, rewards last episode : 470.00 \n",
            "Epsilon 0.6295000000000028\n",
            "\n",
            "End of episode 91, rewards last episode : 390.00 \n",
            "Epsilon 0.6230000000000029\n",
            "target params replaced\n",
            "\n",
            "End of episode 92, rewards last episode : 290.00 \n",
            "\n",
            "End of episode 93, rewards last episode : 520.00 \n",
            "Epsilon 0.6165000000000029\n",
            "\n",
            "End of episode 94, rewards last episode : 280.00 \n",
            "Epsilon 0.610000000000003\n",
            "\n",
            "End of episode 95, rewards last episode : 470.00 \n",
            "Epsilon 0.603500000000003\n",
            "target params replaced\n",
            "\n",
            "End of episode 96, rewards last episode : 330.00 \n",
            "\n",
            "End of episode 97, rewards last episode : 630.00 \n",
            "Epsilon 0.5970000000000031\n",
            "Epsilon 0.5905000000000031\n",
            "\n",
            "End of episode 98, rewards last episode : 320.00 \n",
            "\n",
            "End of episode 99, rewards last episode : 310.00 \n",
            "Epsilon 0.5840000000000032\n",
            "target params replaced\n",
            "\n",
            "End of episode 100, rewards last episode : 240.00 \n",
            "\n",
            "End of episode 101, rewards last episode : 370.00 \n",
            "Epsilon 0.5775000000000032\n",
            "\n",
            "End of episode 102, rewards last episode : 280.00 \n",
            "\n",
            "End of episode 103, rewards last episode : 520.00 \n",
            "Epsilon 0.5710000000000033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNnGPNoAJxky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}